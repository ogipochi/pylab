{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "LOG_DIR = \"./logs\"\n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_LAYER_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/coly/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "with codecs.open(os.path.join(DATA_DIR,\"umich-sentiment-train.txt\"),\"r\",\"utf-8\") as ftrain:\n",
    "    for line in ftrain:\n",
    "        label , sentence = line.strip().split(\"\\t\")    #水平タブで分割\n",
    "        words = nltk.word_tokenize(sentence.lower())   # 単語の表現を正規化\n",
    "        maxlen = max(maxlen,len(words))                # 最大maxlenを更新していく\n",
    "        # すべての文章について出現する単語の数を数えておく\n",
    "        for word in words:\n",
    "            word_freqs[word] += 1\n",
    "            \n",
    "        num_recs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "2326\n",
      "7086\n"
     ]
    }
   ],
   "source": [
    "print(maxlen)\n",
    "print(len(word_freqs))\n",
    "print(num_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 2000\n",
    "MAX_SENTENCE_LENGTH = 40\n",
    "# 上の値はあくまで今回決めたパラメータである\n",
    "# コーパスの数とMAX_FEATURESを比較して小さい方を語彙の数として定義する\n",
    "vocab_size = min(MAX_FEATURES,len(word_freqs)) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = dict()\n",
    "index2word = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate(word_freqs.most_common(MAX_FEATURES)):\n",
    "    word2index[x[0]] = i+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"PAD\" : パディングのための値\n",
    "# \"UNK\" : 語彙にない単語,読み取れない単語（記号とか？）\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in word2index.items():\n",
    "    index2word[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.empty((num_recs,),dtype=list)\n",
    "y = np.zeros((num_recs,))\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章をコーパスから作成した語彙リストのidのリストに変換する\n",
    "X = np.empty((num_recs, ), dtype=list)\n",
    "y = np.zeros((num_recs, ))\n",
    "i = 0\n",
    "with codecs.open(os.path.join(DATA_DIR, \"umich-sentiment-train.txt\"),'r', 'utf-8') as ftrain:\n",
    "    for line in ftrain:\n",
    "        # データはタブ(\\t)でラベルと文章に区切られている\n",
    "        label, sentence = line.strip().split(\"\\t\")\n",
    "        # 単語を正規化したリストを作成\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        seqs = []\n",
    "        # 語彙のリストに入っていればその単語のidを入れ\n",
    "        # なければ\"unknown\"を意味するUNKのidを入れる\n",
    "        for word in words:\n",
    "            if word in word2index:\n",
    "                seqs.append(word2index[word])\n",
    "            else:\n",
    "                seqs.append(word2index[\"UNK\"])\n",
    "        X[i] = seqs\n",
    "        y[i] = int(label)\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力データをすべてmaxlenを最大長にする\n",
    "# 最大長に満たないものは0でパディングする\n",
    "X = sequence.pad_sequences(X,maxlen=MAX_SENTENCE_LENGTH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを8:2で分割する\n",
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを作成する\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,EMBEDDING_SIZE,input_length=MAX_SENTENCE_LENGTH))\n",
    "model.add(LSTM(HIDDEN_LAYER_SIZE,dropout=0.5,recurrent_dropout=0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5668 samples, validate on 1418 samples\n",
      "Epoch 1/10\n",
      "5668/5668 [==============================] - 4s 705us/step - loss: 0.2862 - acc: 0.8716 - val_loss: 0.0759 - val_acc: 0.9760\n",
      "Epoch 2/10\n",
      "5668/5668 [==============================] - 4s 623us/step - loss: 0.0377 - acc: 0.9880 - val_loss: 0.0459 - val_acc: 0.9845\n",
      "Epoch 3/10\n",
      "5668/5668 [==============================] - 4s 629us/step - loss: 0.0120 - acc: 0.9966 - val_loss: 0.0423 - val_acc: 0.9866\n",
      "Epoch 4/10\n",
      "5668/5668 [==============================] - 4s 631us/step - loss: 0.0065 - acc: 0.9979 - val_loss: 0.0599 - val_acc: 0.9810\n",
      "Epoch 5/10\n",
      "5668/5668 [==============================] - 4s 624us/step - loss: 0.0051 - acc: 0.9986 - val_loss: 0.0438 - val_acc: 0.9901\n",
      "Epoch 6/10\n",
      "5668/5668 [==============================] - 4s 630us/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.0498 - val_acc: 0.9894\n",
      "Epoch 7/10\n",
      "5668/5668 [==============================] - 4s 628us/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0630 - val_acc: 0.9901\n",
      "Epoch 8/10\n",
      "5668/5668 [==============================] - 4s 624us/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.0587 - val_acc: 0.9866\n",
      "Epoch 9/10\n",
      "5668/5668 [==============================] - 4s 627us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0688 - val_acc: 0.9887\n",
      "Epoch 10/10\n",
      "5668/5668 [==============================] - 4s 622us/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.0727 - val_acc: 0.9774\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(Xtrain,ytrain,batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,callbacks=[TensorBoard(LOG_DIR)],validation_data=(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1418/1418 [==============================] - 0s 141us/step\n",
      "Test score: 0.07267036836013745,accuracy: 0.9774330040631745\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(Xtest,ytest,batch_size=BATCH_SIZE)\n",
    "print(\"Test score: {},accuracy: {}\".format(score,acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\toh , and brokeback mountain is a terrible movie ...\n",
      "0\t0\ti hate harry potter , it 's retarted , gay and stupid and there 's only one black guy ...\n",
      "1\t1\ti am going to start reading the harry potter series again because that is one awesome story .\n",
      "1\t0\tthen we drove to bayers lake for the da vinci code , which as expected , tom hanks sucks ass in that movie , but the dramatic last 2 minutes were good .\n",
      "1\t1\ti love brokeback mountain .\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,40)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent_list = []\n",
    "    for x in xtest[0].tolist():\n",
    "        if x != 0:\n",
    "            sent_list.append(index2word[x])\n",
    "    sent = \" \".join(sent_list)\n",
    "    print(\"{:.0f}\\t{:.0f}\\t{}\".format(ypred,ylabel,sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists(\"./data/umich-sentiment-tests.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
